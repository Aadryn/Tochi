# TÃ¢che 062 - ImplÃ©menter Abstraction Multi-Providers LLM

## PRIORITÃ‰
ğŸŸ¡ **P3 - MOYENNE** (PrioritÃ© 7/8 de la refonte)

## OBJECTIF

CrÃ©er une couche d'abstraction uniforme pour 9 providers LLM (Ollama, vLLM, OVH Cloud, Scaleway, OpenAI, Azure OpenAI, AWS Bedrock, Anthropic, Mistral AI), permettant le routage intelligent et le failover.

## CONTEXTE

### Providers Cibles
| Provider | Type | API Format | Auth |
|----------|------|------------|------|
| Ollama | Local | Ollama native | None |
| vLLM | Local | OpenAI-compatible | API Key |
| OVH Cloud AI | Private Cloud | OpenAI-compatible | Token |
| Scaleway Generative | Private Cloud | OpenAI-compatible | Secret Key |
| OpenAI | Public Cloud | OpenAI native | API Key |
| Azure OpenAI | Public Cloud | OpenAI-compatible | API Key + Endpoint |
| AWS Bedrock | Public Cloud | AWS native | IAM/SigV4 |
| Anthropic | Public Cloud | Anthropic native | API Key |
| Mistral AI | Public Cloud | OpenAI-compatible | API Key |

### Patterns d'Architecture
- **Strategy Pattern** : Un ILLMProvider par provider
- **Factory Pattern** : CrÃ©ation des clients selon configuration
- **Adapter Pattern** : Normalisation des rÃ©ponses

## IMPLÃ‰MENTATION

### Phase 1 : Domain Layer (Abstractions)
```
src/Core/LLMProxy.Domain/
â”œâ”€â”€ Interfaces/
â”‚   â”œâ”€â”€ ILLMProvider.cs           # Interface commune
â”‚   â””â”€â”€ ILLMProviderFactory.cs
â”œâ”€â”€ Entities/LLM/
â”‚   â”œâ”€â”€ LLMRequest.cs             # RequÃªte normalisÃ©e
â”‚   â”œâ”€â”€ LLMResponse.cs            # RÃ©ponse normalisÃ©e
â”‚   â”œâ”€â”€ LLMMessage.cs             # Message chat
â”‚   â”œâ”€â”€ LLMModel.cs               # ModÃ¨le disponible
â”‚   â””â”€â”€ ProviderCapabilities.cs   # CapacitÃ©s du provider
â”œâ”€â”€ ValueObjects/
â”‚   â”œâ”€â”€ ModelIdentifier.cs        # "gpt-4", "claude-3-opus"
â”‚   â”œâ”€â”€ ProviderType.cs           # Enum des providers
â”‚   â””â”€â”€ TokenUsage.cs             # Input/Output tokens
â””â”€â”€ Events/
    â”œâ”€â”€ LLMRequestStarted.cs
    â”œâ”€â”€ LLMRequestCompleted.cs
    â””â”€â”€ LLMRequestFailed.cs
```

```csharp
// ILLMProvider.cs
public interface ILLMProvider
{
    ProviderType Type { get; }
    string Name { get; }
    
    Task<ProviderCapabilities> GetCapabilitiesAsync(CancellationToken ct = default);
    Task<IReadOnlyList<LLMModel>> ListModelsAsync(CancellationToken ct = default);
    Task<bool> IsHealthyAsync(CancellationToken ct = default);
    
    Task<LLMResponse> ChatCompletionAsync(LLMRequest request, CancellationToken ct = default);
    IAsyncEnumerable<LLMResponse> ChatCompletionStreamAsync(LLMRequest request, CancellationToken ct = default);
    Task<EmbeddingResponse> EmbeddingsAsync(EmbeddingRequest request, CancellationToken ct = default);
}
```

### Phase 2 : Infrastructure Layer (ImplÃ©mentations)
```
src/Infrastructure/LLMProxy.Infrastructure.LLMProviders/
â”œâ”€â”€ Providers/
â”‚   â”œâ”€â”€ Local/
â”‚   â”‚   â”œâ”€â”€ OllamaProvider.cs
â”‚   â”‚   â””â”€â”€ VLLMProvider.cs
â”‚   â”œâ”€â”€ PrivateCloud/
â”‚   â”‚   â”œâ”€â”€ OVHCloudProvider.cs
â”‚   â”‚   â””â”€â”€ ScalewayProvider.cs
â”‚   â””â”€â”€ PublicCloud/
â”‚       â”œâ”€â”€ OpenAIProvider.cs
â”‚       â”œâ”€â”€ AzureOpenAIProvider.cs
â”‚       â”œâ”€â”€ AWSBedrockProvider.cs
â”‚       â”œâ”€â”€ AnthropicProvider.cs
â”‚       â””â”€â”€ MistralProvider.cs
â”œâ”€â”€ Adapters/
â”‚   â”œâ”€â”€ OpenAIFormatAdapter.cs    # Convertit vers format OpenAI
â”‚   â”œâ”€â”€ AnthropicFormatAdapter.cs
â”‚   â””â”€â”€ BedrockFormatAdapter.cs
â”œâ”€â”€ Clients/
â”‚   â”œâ”€â”€ HttpClientFactory.cs
â”‚   â””â”€â”€ RetryPolicyHandler.cs
â”œâ”€â”€ Configuration/
â”‚   â”œâ”€â”€ ProviderConfiguration.cs
â”‚   â””â”€â”€ ProviderCredentials.cs
â””â”€â”€ Factory/
    â””â”€â”€ LLMProviderFactory.cs
```

### Phase 3 : Application Layer (Orchestration)
```
src/Application/LLMProxy.Application/
â”œâ”€â”€ Features/
â”‚   â””â”€â”€ Providers/
â”‚       â”œâ”€â”€ Services/
â”‚       â”‚   â”œâ”€â”€ ProviderOrchestrator.cs    # Routing intelligent
â”‚       â”‚   â”œâ”€â”€ ProviderSelector.cs        # SÃ©lection par critÃ¨res
â”‚       â”‚   â””â”€â”€ FailoverManager.cs         # Gestion failover
â”‚       â””â”€â”€ Queries/
â”‚           â”œâ”€â”€ GetBestProvider/
â”‚           â””â”€â”€ GetProviderMetrics/
```

### Phase 4 : Configuration
```json
{
  "LLMProviders": {
    "Providers": [
      {
        "Type": "OpenAI",
        "Name": "openai-primary",
        "Enabled": true,
        "Priority": 1,
        "Configuration": {
          "BaseUrl": "https://api.openai.com/v1",
          "ApiKey": "${OPENAI_API_KEY}",
          "DefaultModel": "gpt-4o",
          "MaxTokens": 4096
        }
      },
      {
        "Type": "Ollama",
        "Name": "ollama-local",
        "Enabled": true,
        "Priority": 2,
        "Configuration": {
          "BaseUrl": "http://localhost:11434",
          "DefaultModel": "llama3.1"
        }
      },
      {
        "Type": "AzureOpenAI",
        "Name": "azure-eu",
        "Enabled": true,
        "Priority": 3,
        "Configuration": {
          "Endpoint": "https://myresource.openai.azure.com/",
          "ApiKey": "${AZURE_OPENAI_KEY}",
          "DeploymentName": "gpt-4o-deployment",
          "ApiVersion": "2024-02-01"
        }
      }
    ],
    "Routing": {
      "Strategy": "PriorityWithFallback",
      "HealthCheckInterval": "00:00:30",
      "FailoverThreshold": 3
    }
  }
}
```

## CRITÃˆRES DE SUCCÃˆS

- [x] Interface ILLMProvider avec 6 mÃ©thodes (ILLMProviderClient)
- [x] 9 providers implÃ©mentÃ©s (Ollama, vLLM, OVH, Scaleway, OpenAI, AzureOpenAI, Bedrock, Anthropic, Mistral)
- [x] Normalisation request/response (LLMRequest/LLMResponse)
- [x] Streaming support (IAsyncEnumerable<LLMResponse>)
- [x] Factory pattern pour crÃ©ation providers (ILLMProviderClientFactory)
- [x] Health checks par provider (IsHealthyAsync)
- [x] Failover automatique (FailoverManager)
- [x] Configuration externalisÃ©e (ProviderSettings, DI extensions)
- [x] Tests unitaires services (21 tests passent)
- [ ] Tests d'intÃ©gration avec Ollama local (non requis immÃ©diatement)
- [x] Build : 0 erreurs, 0 warnings
- [x] Documentation XML complÃ¨te (franÃ§ais)

## RAPPORT DE COMPLÃ‰TION

### Fichiers CrÃ©Ã©s

#### Phase 1 : Domain Layer (Abstractions)
- `src/Core/LLMProxy.Domain/LLM/LLMRequest.cs` - RequÃªte normalisÃ©e
- `src/Core/LLMProxy.Domain/LLM/LLMResponse.cs` - RÃ©ponse normalisÃ©e  
- `src/Core/LLMProxy.Domain/LLM/LLMMessage.cs` - Message chat
- `src/Core/LLMProxy.Domain/LLM/LLMModel.cs` - ModÃ¨le disponible
- `src/Core/LLMProxy.Domain/LLM/ModelIdentifier.cs` - Value Object
- `src/Core/LLMProxy.Domain/LLM/TokenUsage.cs` - Usage tokens
- `src/Core/LLMProxy.Domain/LLM/EmbeddingRequest.cs` - RequÃªte embeddings
- `src/Core/LLMProxy.Domain/LLM/EmbeddingResponse.cs` - RÃ©ponse embeddings
- `src/Core/LLMProxy.Domain/LLM/ILLMProviderClient.cs` - Interface commune
- `src/Core/LLMProxy.Domain/Entities/ProviderType.cs` - Enum 9 providers
- `src/Core/LLMProxy.Domain/Entities/ProviderCapabilities.cs` - CapacitÃ©s provider
- `src/Core/LLMProxy.Domain/Entities/ModelCapabilities.cs` - CapacitÃ©s modÃ¨le

#### Phase 2 : Infrastructure Layer (9 Providers)
- `src/Infrastructure/LLMProxy.Infrastructure.LLMProviders/LLMProxy.Infrastructure.LLMProviders.csproj`
- **Providers Local:**
  - `Providers/Local/OllamaProviderClient.cs`
  - `Providers/Local/VLLMProviderClient.cs`
- **Providers Private Cloud:**
  - `Providers/PrivateCloud/OVHCloudProviderClient.cs`
  - `Providers/PrivateCloud/ScalewayProviderClient.cs`
- **Providers Public Cloud:**
  - `Providers/PublicCloud/OpenAIProviderClient.cs`
  - `Providers/PublicCloud/AzureOpenAIProviderClient.cs`
  - `Providers/PublicCloud/AWSBedrockProviderClient.cs`
  - `Providers/PublicCloud/AnthropicProviderClient.cs`
  - `Providers/PublicCloud/MistralProviderClient.cs`
- **Factory:**
  - `Factory/ILLMProviderClientFactory.cs`
  - `Factory/LLMProviderClientFactory.cs`
- **Configuration:**
  - `Configuration/ProviderSettings.cs`
- **Extensions:**
  - `Extensions/LLMProvidersServiceCollectionExtensions.cs`

#### Phase 3 : Application Layer (Orchestration)
- `src/Application/LLMProxy.Application/LLMProviders/Services/IProviderSelector.cs`
- `src/Application/LLMProxy.Application/LLMProviders/Services/ProviderSelector.cs`
- `src/Application/LLMProxy.Application/LLMProviders/Services/SelectionCriteria.cs`
- `src/Application/LLMProxy.Application/LLMProviders/Services/FailoverManager.cs` (IFailoverManager + FailoverResult + FailoverOptions)
- `src/Application/LLMProxy.Application/LLMProviders/Services/ProviderOrchestrator.cs` (IProviderOrchestrator + OrchestratorResult + ExecutionContext)

#### Phase 4 : DI Extensions
- `src/Application/LLMProxy.Application/Extensions/LLMProvidersApplicationServiceCollectionExtensions.cs`

#### Phase 5 : Tests
- `tests/LLMProxy.Application.Tests/LLMProviders/Services/FailoverManagerTests.cs` (11 tests)
- `tests/LLMProxy.Application.Tests/LLMProviders/Services/ProviderSelectorTests.cs` (10 tests)

### RÃ©sultats Validation

- **Build:** âœ… 0 erreurs, 0 warnings
- **Tests:** âœ… 21/21 tests passent (nouveaux tests LLM)
- **Tests globaux:** 96/96 tests Application passent

### Architecture ImplÃ©mentÃ©e

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Application Layer                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ProviderOrchestratorâ”‚  â”‚ProviderSelectorâ”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚           â”‚                     â”‚                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚                        â”‚
â”‚  â”‚ FailoverManager â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               Infrastructure Layer                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚           LLMProviderClientFactory              â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                      â”‚                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              9 Provider Clients                  â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚    â”‚
â”‚  â”‚  â”‚ Ollama  â”‚ â”‚  vLLM   â”‚ â”‚  OVH    â”‚          â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚    â”‚
â”‚  â”‚  â”‚Scaleway â”‚ â”‚ OpenAI  â”‚ â”‚Azure OAIâ”‚          â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚    â”‚
â”‚  â”‚  â”‚ Bedrock â”‚ â”‚Anthropicâ”‚ â”‚ Mistral â”‚          â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Domain Layer                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ ILLMProviderClient â”‚ LLMRequest â”‚ LLMResponse     â”‚  â”‚
â”‚  â”‚ ProviderType      â”‚ TokenUsage â”‚ ModelIdentifier  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## DÃ‰PENDANCES

- Ollama installÃ© localement pour tests
- Credentials des providers cloud (variables env)

## ESTIMATION

**Effort** : 20h (9 providers)
**ComplexitÃ©** : Haute

## RÃ‰FÃ‰RENCES

- [OpenAI API](https://platform.openai.com/docs/api-reference)
- [Anthropic API](https://docs.anthropic.com/claude/reference)
- [AWS Bedrock](https://docs.aws.amazon.com/bedrock/)
- [Ollama API](https://github.com/ollama/ollama/blob/main/docs/api.md)
- ADR-034 (Third-Party Library Encapsulation)

## TRACKING
DÃ©but: 2025-01-07T18:50:00Z
Fin: 2025-01-08T10:15:00Z
DurÃ©e: ~15h30

**Statut: âœ… COMPLÃ‰TÃ‰**
