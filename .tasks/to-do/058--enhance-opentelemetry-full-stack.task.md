# TÃ¢che 058 - AmÃ©liorer OpenTelemetry Full Stack

## PRIORITÃ‰
ðŸ”´ **P1 - HAUTE** (PrioritÃ© 3/8 de la refonte)

## OBJECTIF

Ã‰tendre l'intÃ©gration OpenTelemetry existante vers une observabilitÃ© complÃ¨te : traces distribuÃ©es enrichies, mÃ©triques mÃ©tier LLM, logs corrÃ©lÃ©s, et export vers collecteur OTLP.

## CONTEXTE

### Ã‰tat Actuel
- OpenTelemetry 1.9.0 intÃ©grÃ© (traces + mÃ©triques de base)
- Instrumentation ASP.NET Core et HttpClient
- Export Console + OTLP basique

### Ã‰tat Cible
- Traces enrichies avec contexte LLM (model, provider, tokens)
- MÃ©triques mÃ©tier (latence par provider, tokens/sec, coÃ»t estimÃ©)
- CorrÃ©lation logs-traces-mÃ©triques via TraceId/SpanId
- Baggage propagation pour contexte multi-service
- Dashboards prÃªts pour Grafana/Jaeger

## IMPLÃ‰MENTATION

### Phase 1 : Custom Instrumentation LLM
```
src/Infrastructure/LLMProxy.Infrastructure.Telemetry/
â”œâ”€â”€ Tracing/
â”‚   â”œâ”€â”€ LLMActivitySource.cs       # ActivitySource dÃ©diÃ© LLM
â”‚   â”œâ”€â”€ LLMSpanProcessor.cs        # Enrichissement spans
â”‚   â””â”€â”€ LLMSamplingStrategy.cs     # Sampling adaptatif
â”œâ”€â”€ Metrics/
â”‚   â”œâ”€â”€ LLMMetrics.cs              # Meters custom
â”‚   â”œâ”€â”€ ProviderLatencyHistogram.cs
â”‚   â”œâ”€â”€ TokenCounterGauge.cs
â”‚   â””â”€â”€ CostEstimationCounter.cs
â”œâ”€â”€ Baggage/
â”‚   â””â”€â”€ LLMBaggagePropagator.cs    # Propagation TenantId, etc.
```

### Phase 2 : Enrichissement Automatique
```csharp
// Attributs sÃ©mantiques LLM (conventions OpenTelemetry GenAI)
public static class LLMSemanticConventions
{
    public const string GEN_AI_SYSTEM = "gen_ai.system";           // "openai", "anthropic"
    public const string GEN_AI_REQUEST_MODEL = "gen_ai.request.model";
    public const string GEN_AI_RESPONSE_MODEL = "gen_ai.response.model";
    public const string GEN_AI_REQUEST_MAX_TOKENS = "gen_ai.request.max_tokens";
    public const string GEN_AI_USAGE_INPUT_TOKENS = "gen_ai.usage.input_tokens";
    public const string GEN_AI_USAGE_OUTPUT_TOKENS = "gen_ai.usage.output_tokens";
    public const string GEN_AI_RESPONSE_FINISH_REASONS = "gen_ai.response.finish_reasons";
}
```

### Phase 3 : MÃ©triques MÃ©tier
```csharp
// MÃ©triques Ã  implÃ©menter
- llmproxy.request.duration (Histogram, par provider/model)
- llmproxy.request.count (Counter, par tenant/provider/status)
- llmproxy.tokens.input (Counter, par tenant/model)
- llmproxy.tokens.output (Counter, par tenant/model)
- llmproxy.cost.estimated (Counter, par tenant, en centimes)
- llmproxy.provider.availability (Gauge, 0/1 par provider)
- llmproxy.ratelimit.remaining (Gauge, par tenant)
```

### Phase 4 : Configuration AvancÃ©e
```json
{
  "OpenTelemetry": {
    "ServiceName": "LLMProxy.Gateway",
    "ServiceVersion": "2.0.0",
    "Tracing": {
      "Sampler": "ParentBased",
      "SamplingRatio": 0.1,
      "AlwaysSampleErrors": true
    },
    "Metrics": {
      "ExportIntervalMilliseconds": 15000,
      "HistogramBuckets": [0.01, 0.05, 0.1, 0.5, 1, 2, 5, 10, 30, 60]
    },
    "Exporter": {
      "Type": "OTLP",
      "Endpoint": "http://otel-collector:4317",
      "Protocol": "grpc"
    }
  }
}
```

## CRITÃˆRES DE SUCCÃˆS

- [ ] ActivitySource dÃ©diÃ© `LLMProxy.LLM` crÃ©Ã©
- [ ] MÃ©triques mÃ©tier LLM implÃ©mentÃ©es (7 mÃ©triques)
- [ ] Attributs sÃ©mantiques GenAI sur tous les spans LLM
- [ ] CorrÃ©lation TraceId dans logs Serilog
- [ ] Baggage TenantId/RequestId propagÃ©
- [ ] Configuration OTLP externalisÃ©e
- [ ] Tests unitaires pour mÃ©triques
- [ ] Build : 0 erreurs, 0 warnings
- [ ] Documentation XML complÃ¨te (franÃ§ais)

## DÃ‰PENDANCES

- TÃ¢che 057 (Serilog) pour corrÃ©lation logs

## ESTIMATION

**Effort** : 6h
**ComplexitÃ©** : Moyenne

## RÃ‰FÃ‰RENCES

- [OpenTelemetry Semantic Conventions GenAI](https://opentelemetry.io/docs/specs/semconv/gen-ai/)
- [OpenTelemetry .NET](https://opentelemetry.io/docs/languages/net/)
- ADR existants OpenTelemetry
