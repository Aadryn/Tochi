# TÃ¢che 063 - ImplÃ©menter Support Multi-Format API (OpenAI + Ollama)

## PRIORITÃ‰
ðŸŸ¡ **P3 - MOYENNE** (PrioritÃ© 8/8 de la refonte)

## OBJECTIF

ImplÃ©menter un systÃ¨me de transformation de requÃªtes permettant aux clients d'utiliser soit le format API OpenAI, soit le format API Ollama, avec conversion transparente vers le format interne et routage vers n'importe quel provider backend.

## CONTEXTE

### Formats API SupportÃ©s en EntrÃ©e
1. **Format OpenAI** : Standard de facto, utilisÃ© par la plupart des clients
2. **Format Ollama** : Populaire pour les dÃ©ploiements locaux

### Principe
```
Client (OpenAI format) â†’ Gateway â†’ Normalisation â†’ Provider (any format)
Client (Ollama format) â†’ Gateway â†’ Normalisation â†’ Provider (any format)
```

### Endpoints ExposÃ©s
```
# OpenAI-compatible endpoints
POST /v1/chat/completions
POST /v1/completions
POST /v1/embeddings
GET  /v1/models

# Ollama-compatible endpoints
POST /api/chat
POST /api/generate
POST /api/embeddings
GET  /api/tags
```

## IMPLÃ‰MENTATION

### Phase 1 : Domain Layer (ModÃ¨les Canoniques)
```
src/Core/LLMProxy.Domain/
â”œâ”€â”€ Models/
â”‚   â”œâ”€â”€ Canonical/
â”‚   â”‚   â”œâ”€â”€ CanonicalChatRequest.cs
â”‚   â”‚   â”œâ”€â”€ CanonicalChatResponse.cs
â”‚   â”‚   â”œâ”€â”€ CanonicalEmbeddingRequest.cs
â”‚   â”‚   â””â”€â”€ CanonicalEmbeddingResponse.cs
â”‚   â””â”€â”€ Formats/
â”‚       â”œâ”€â”€ ApiFormat.cs          # Enum: OpenAI, Ollama
â”‚       â””â”€â”€ ContentType.cs        # Text, Image, Audio
```

### Phase 2 : Application Layer (Transformers)
```
src/Application/LLMProxy.Application/
â”œâ”€â”€ Features/
â”‚   â””â”€â”€ ApiTransformation/
â”‚       â”œâ”€â”€ Interfaces/
â”‚       â”‚   â”œâ”€â”€ IRequestTransformer.cs
â”‚       â”‚   â””â”€â”€ IResponseTransformer.cs
â”‚       â”œâ”€â”€ Transformers/
â”‚       â”‚   â”œâ”€â”€ OpenAI/
â”‚       â”‚   â”‚   â”œâ”€â”€ OpenAIRequestTransformer.cs
â”‚       â”‚   â”‚   â””â”€â”€ OpenAIResponseTransformer.cs
â”‚       â”‚   â””â”€â”€ Ollama/
â”‚       â”‚       â”œâ”€â”€ OllamaRequestTransformer.cs
â”‚       â”‚       â””â”€â”€ OllamaResponseTransformer.cs
â”‚       â”œâ”€â”€ Factory/
â”‚       â”‚   â””â”€â”€ TransformerFactory.cs
â”‚       â””â”€â”€ Services/
â”‚           â””â”€â”€ ApiFormatDetector.cs   # DÃ©tection auto du format
```

### Phase 3 : Format OpenAI (RÃ©fÃ©rence)
```csharp
// OpenAI Chat Request
{
  "model": "gpt-4o",
  "messages": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"}
  ],
  "temperature": 0.7,
  "max_tokens": 1000,
  "stream": false
}

// OpenAI Chat Response
{
  "id": "chatcmpl-xxx",
  "object": "chat.completion",
  "created": 1234567890,
  "model": "gpt-4o",
  "choices": [{
    "index": 0,
    "message": {"role": "assistant", "content": "Hello! How can I help?"},
    "finish_reason": "stop"
  }],
  "usage": {
    "prompt_tokens": 20,
    "completion_tokens": 10,
    "total_tokens": 30
  }
}
```

### Phase 4 : Format Ollama (Transformation)
```csharp
// Ollama Chat Request (entrÃ©e)
{
  "model": "llama3.1",
  "messages": [
    {"role": "user", "content": "Hello!"}
  ],
  "stream": false,
  "options": {
    "temperature": 0.7,
    "num_predict": 1000
  }
}

// Transformation vers Canonical
public class OllamaRequestTransformer : IRequestTransformer
{
    public CanonicalChatRequest Transform(JsonDocument ollamaRequest)
    {
        return new CanonicalChatRequest
        {
            Model = ollamaRequest.RootElement.GetProperty("model").GetString(),
            Messages = TransformMessages(ollamaRequest),
            Temperature = GetNestedOption<float>("temperature", 0.7f),
            MaxTokens = GetNestedOption<int>("num_predict", 1000),
            Stream = ollamaRequest.RootElement.GetProperty("stream").GetBoolean()
        };
    }
}
```

### Phase 5 : Gateway Controllers
```
src/Presentation/LLMProxy.Gateway/
â”œâ”€â”€ Controllers/
â”‚   â”œâ”€â”€ V1/
â”‚   â”‚   â”œâ”€â”€ ChatCompletionsController.cs     # OpenAI format
â”‚   â”‚   â”œâ”€â”€ CompletionsController.cs
â”‚   â”‚   â”œâ”€â”€ EmbeddingsController.cs
â”‚   â”‚   â””â”€â”€ ModelsController.cs
â”‚   â””â”€â”€ Ollama/
â”‚       â”œâ”€â”€ OllamaChatController.cs          # Ollama format
â”‚       â”œâ”€â”€ OllamaGenerateController.cs
â”‚       â”œâ”€â”€ OllamaEmbeddingsController.cs
â”‚       â””â”€â”€ OllamaTagsController.cs
â”œâ”€â”€ Middleware/
â”‚   â””â”€â”€ ApiFormatDetectionMiddleware.cs
```

### Phase 6 : Streaming Support
```csharp
// SSE streaming pour OpenAI format
app.MapPost("/v1/chat/completions", async (HttpContext ctx, ...) =>
{
    if (request.Stream)
    {
        ctx.Response.ContentType = "text/event-stream";
        await foreach (var chunk in provider.ChatCompletionStreamAsync(request))
        {
            var openAiChunk = transformer.TransformToOpenAI(chunk);
            await ctx.Response.WriteAsync($"data: {JsonSerializer.Serialize(openAiChunk)}\n\n");
        }
        await ctx.Response.WriteAsync("data: [DONE]\n\n");
    }
});
```

## CRITÃˆRES DE SUCCÃˆS

- [ ] CanonicalChatRequest/Response crÃ©Ã©s
- [ ] OpenAI transformer implÃ©mentÃ© (request + response)
- [ ] Ollama transformer implÃ©mentÃ© (request + response)
- [ ] 4 endpoints OpenAI fonctionnels
- [ ] 4 endpoints Ollama fonctionnels
- [ ] Streaming SSE pour les deux formats
- [ ] DÃ©tection automatique du format
- [ ] Tests unitaires transformers
- [ ] Tests d'intÃ©gration endpoints
- [ ] Documentation OpenAPI/Swagger
- [ ] Build : 0 erreurs, 0 warnings
- [ ] Documentation XML complÃ¨te (franÃ§ais)

## DÃ‰PENDANCES

- TÃ¢che 062 (Multi-Provider LLM) pour le backend

## ESTIMATION

**Effort** : 10h
**ComplexitÃ©** : Moyenne

## RÃ‰FÃ‰RENCES

- [OpenAI API Reference](https://platform.openai.com/docs/api-reference/chat)
- [Ollama API Reference](https://github.com/ollama/ollama/blob/main/docs/api.md)
- ADR-037 (API Versioning Strategy)
