# TÃ¢che 062 - ImplÃ©menter Abstraction Multi-Providers LLM

## PRIORITÃ‰
ðŸŸ¡ **P3 - MOYENNE** (PrioritÃ© 7/8 de la refonte)

## OBJECTIF

CrÃ©er une couche d'abstraction uniforme pour 9 providers LLM (Ollama, vLLM, OVH Cloud, Scaleway, OpenAI, Azure OpenAI, AWS Bedrock, Anthropic, Mistral AI), permettant le routage intelligent et le failover.

## CONTEXTE

### Providers Cibles
| Provider | Type | API Format | Auth |
|----------|------|------------|------|
| Ollama | Local | Ollama native | None |
| vLLM | Local | OpenAI-compatible | API Key |
| OVH Cloud AI | Private Cloud | OpenAI-compatible | Token |
| Scaleway Generative | Private Cloud | OpenAI-compatible | Secret Key |
| OpenAI | Public Cloud | OpenAI native | API Key |
| Azure OpenAI | Public Cloud | OpenAI-compatible | API Key + Endpoint |
| AWS Bedrock | Public Cloud | AWS native | IAM/SigV4 |
| Anthropic | Public Cloud | Anthropic native | API Key |
| Mistral AI | Public Cloud | OpenAI-compatible | API Key |

### Patterns d'Architecture
- **Strategy Pattern** : Un ILLMProvider par provider
- **Factory Pattern** : CrÃ©ation des clients selon configuration
- **Adapter Pattern** : Normalisation des rÃ©ponses

## IMPLÃ‰MENTATION

### Phase 1 : Domain Layer (Abstractions)
```
src/Core/LLMProxy.Domain/
â”œâ”€â”€ Interfaces/
â”‚   â”œâ”€â”€ ILLMProvider.cs           # Interface commune
â”‚   â””â”€â”€ ILLMProviderFactory.cs
â”œâ”€â”€ Entities/LLM/
â”‚   â”œâ”€â”€ LLMRequest.cs             # RequÃªte normalisÃ©e
â”‚   â”œâ”€â”€ LLMResponse.cs            # RÃ©ponse normalisÃ©e
â”‚   â”œâ”€â”€ LLMMessage.cs             # Message chat
â”‚   â”œâ”€â”€ LLMModel.cs               # ModÃ¨le disponible
â”‚   â””â”€â”€ ProviderCapabilities.cs   # CapacitÃ©s du provider
â”œâ”€â”€ ValueObjects/
â”‚   â”œâ”€â”€ ModelIdentifier.cs        # "gpt-4", "claude-3-opus"
â”‚   â”œâ”€â”€ ProviderType.cs           # Enum des providers
â”‚   â””â”€â”€ TokenUsage.cs             # Input/Output tokens
â””â”€â”€ Events/
    â”œâ”€â”€ LLMRequestStarted.cs
    â”œâ”€â”€ LLMRequestCompleted.cs
    â””â”€â”€ LLMRequestFailed.cs
```

```csharp
// ILLMProvider.cs
public interface ILLMProvider
{
    ProviderType Type { get; }
    string Name { get; }
    
    Task<ProviderCapabilities> GetCapabilitiesAsync(CancellationToken ct = default);
    Task<IReadOnlyList<LLMModel>> ListModelsAsync(CancellationToken ct = default);
    Task<bool> IsHealthyAsync(CancellationToken ct = default);
    
    Task<LLMResponse> ChatCompletionAsync(LLMRequest request, CancellationToken ct = default);
    IAsyncEnumerable<LLMResponse> ChatCompletionStreamAsync(LLMRequest request, CancellationToken ct = default);
    Task<EmbeddingResponse> EmbeddingsAsync(EmbeddingRequest request, CancellationToken ct = default);
}
```

### Phase 2 : Infrastructure Layer (ImplÃ©mentations)
```
src/Infrastructure/LLMProxy.Infrastructure.LLMProviders/
â”œâ”€â”€ Providers/
â”‚   â”œâ”€â”€ Local/
â”‚   â”‚   â”œâ”€â”€ OllamaProvider.cs
â”‚   â”‚   â””â”€â”€ VLLMProvider.cs
â”‚   â”œâ”€â”€ PrivateCloud/
â”‚   â”‚   â”œâ”€â”€ OVHCloudProvider.cs
â”‚   â”‚   â””â”€â”€ ScalewayProvider.cs
â”‚   â””â”€â”€ PublicCloud/
â”‚       â”œâ”€â”€ OpenAIProvider.cs
â”‚       â”œâ”€â”€ AzureOpenAIProvider.cs
â”‚       â”œâ”€â”€ AWSBedrockProvider.cs
â”‚       â”œâ”€â”€ AnthropicProvider.cs
â”‚       â””â”€â”€ MistralProvider.cs
â”œâ”€â”€ Adapters/
â”‚   â”œâ”€â”€ OpenAIFormatAdapter.cs    # Convertit vers format OpenAI
â”‚   â”œâ”€â”€ AnthropicFormatAdapter.cs
â”‚   â””â”€â”€ BedrockFormatAdapter.cs
â”œâ”€â”€ Clients/
â”‚   â”œâ”€â”€ HttpClientFactory.cs
â”‚   â””â”€â”€ RetryPolicyHandler.cs
â”œâ”€â”€ Configuration/
â”‚   â”œâ”€â”€ ProviderConfiguration.cs
â”‚   â””â”€â”€ ProviderCredentials.cs
â””â”€â”€ Factory/
    â””â”€â”€ LLMProviderFactory.cs
```

### Phase 3 : Application Layer (Orchestration)
```
src/Application/LLMProxy.Application/
â”œâ”€â”€ Features/
â”‚   â””â”€â”€ Providers/
â”‚       â”œâ”€â”€ Services/
â”‚       â”‚   â”œâ”€â”€ ProviderOrchestrator.cs    # Routing intelligent
â”‚       â”‚   â”œâ”€â”€ ProviderSelector.cs        # SÃ©lection par critÃ¨res
â”‚       â”‚   â””â”€â”€ FailoverManager.cs         # Gestion failover
â”‚       â””â”€â”€ Queries/
â”‚           â”œâ”€â”€ GetBestProvider/
â”‚           â””â”€â”€ GetProviderMetrics/
```

### Phase 4 : Configuration
```json
{
  "LLMProviders": {
    "Providers": [
      {
        "Type": "OpenAI",
        "Name": "openai-primary",
        "Enabled": true,
        "Priority": 1,
        "Configuration": {
          "BaseUrl": "https://api.openai.com/v1",
          "ApiKey": "${OPENAI_API_KEY}",
          "DefaultModel": "gpt-4o",
          "MaxTokens": 4096
        }
      },
      {
        "Type": "Ollama",
        "Name": "ollama-local",
        "Enabled": true,
        "Priority": 2,
        "Configuration": {
          "BaseUrl": "http://localhost:11434",
          "DefaultModel": "llama3.1"
        }
      },
      {
        "Type": "AzureOpenAI",
        "Name": "azure-eu",
        "Enabled": true,
        "Priority": 3,
        "Configuration": {
          "Endpoint": "https://myresource.openai.azure.com/",
          "ApiKey": "${AZURE_OPENAI_KEY}",
          "DeploymentName": "gpt-4o-deployment",
          "ApiVersion": "2024-02-01"
        }
      }
    ],
    "Routing": {
      "Strategy": "PriorityWithFallback",
      "HealthCheckInterval": "00:00:30",
      "FailoverThreshold": 3
    }
  }
}
```

## CRITÃˆRES DE SUCCÃˆS

- [ ] Interface ILLMProvider avec 6 mÃ©thodes
- [ ] 9 providers implÃ©mentÃ©s
- [ ] Normalisation request/response
- [ ] Streaming support (IAsyncEnumerable)
- [ ] Factory pattern pour crÃ©ation providers
- [ ] Health checks par provider
- [ ] Failover automatique
- [ ] Configuration externalisÃ©e
- [ ] Tests unitaires par provider (mocks HTTP)
- [ ] Tests d'intÃ©gration avec Ollama local
- [ ] Build : 0 erreurs, 0 warnings
- [ ] Documentation XML complÃ¨te (franÃ§ais)

## DÃ‰PENDANCES

- Ollama installÃ© localement pour tests
- Credentials des providers cloud (variables env)

## ESTIMATION

**Effort** : 20h (9 providers)
**ComplexitÃ©** : Haute

## RÃ‰FÃ‰RENCES

- [OpenAI API](https://platform.openai.com/docs/api-reference)
- [Anthropic API](https://docs.anthropic.com/claude/reference)
- [AWS Bedrock](https://docs.aws.amazon.com/bedrock/)
- [Ollama API](https://github.com/ollama/ollama/blob/main/docs/api.md)
- ADR-034 (Third-Party Library Encapsulation)
