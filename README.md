# LLM Proxy

[![Build Status](https://img.shields.io/badge/build-passing-brightgreen)](.)
[![.NET 9](https://img.shields.io/badge/.NET-9.0-512BD4)](https://dotnet.microsoft.com/)
[![Vue.js 3](https://img.shields.io/badge/Vue.js-3.x-4FC08D)](https://vuejs.org/)
[![License](https://img.shields.io/badge/license-MIT-blue)](LICENSE)

> Un proxy intelligent pour les API de modÃ¨les de langage (LLM) avec multi-tenant, multi-provider et gestion avancÃ©e des quotas.

## ğŸ“‹ FonctionnalitÃ©s

- **Multi-Provider** : OpenAI, Anthropic, Azure OpenAI, Ollama, Cohere, Mistral
- **Multi-Format API** : Support des formats OpenAI et Ollama cÃ´tÃ© client
- **Multi-Tenant** : Isolation complÃ¨te par tenant avec configuration indÃ©pendante
- **Gestion des Quotas** : Rate limiting par utilisateur/tenant avec Redis
- **Streaming** : Support complet du streaming SSE
- **ObservabilitÃ©** : OpenTelemetry, mÃ©triques Prometheus, traces distribuÃ©es
- **SÃ©curitÃ©** : Authentification API Key, intÃ©gration Keycloak, JWT
- **Administration** : API REST + Interface d'administration Vue.js

## ğŸ—ï¸ Architecture

```
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚         Clients             â”‚
                            â”‚  (OpenAI SDK / Ollama CLI)  â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â”‚
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚     LLM Proxy Gateway       â”‚
                            â”‚   (YARP Reverse Proxy)      â”‚
                            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
                            â”‚  â”‚ API Format Detectionâ”‚    â”‚
                            â”‚  â”‚ Rate Limiting       â”‚    â”‚
                            â”‚  â”‚ Authentication      â”‚    â”‚
                            â”‚  â”‚ Load Balancing      â”‚    â”‚
                            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚                      â”‚                      â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚      OpenAI         â”‚ â”‚    Anthropic    â”‚ â”‚       Ollama        â”‚
        â”‚    (gpt-4o)         â”‚ â”‚   (claude-3)    â”‚ â”‚    (llama3.1)       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Le projet suit une **Architecture Hexagonale** (Ports & Adapters) avec CQRS et les principes SOLID.

Pour plus de dÃ©tails, voir [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md).

## ğŸš€ DÃ©marrage Rapide

### PrÃ©requis

- Docker & Docker Compose
- .NET 9 SDK
- Node.js 20+
- PowerShell 7+ (Windows) ou Bash (Linux/macOS)

### Installation

```bash
# 1. Cloner le repository
git clone https://github.com/your-org/llm-proxy.git
cd llm-proxy

# 2. DÃ©marrer l'environnement de dÃ©veloppement
docker compose -f .environments/docker-compose.yml up -d

# 3. DÃ©marrer le backend
cd applications/proxy/backend
dotnet restore
dotnet build
dotnet run --project src/Presentation/LLMProxy.Gateway

# 4. DÃ©marrer le frontend (dans un autre terminal)
cd applications/proxy/frontend
npm install
npm run dev
```

### AccÃ¨s aux Services

| Service | URL | Description |
|---------|-----|-------------|
| Gateway API | http://localhost:5000 | API proxy LLM |
| Admin API | http://localhost:5001 | API d'administration |
| Admin UI | http://localhost:3000 | Interface d'administration |
| Keycloak | http://localhost:8080 | Identity Provider |
| Grafana | http://localhost:3001 | Dashboards de monitoring |
| Jaeger | http://localhost:16686 | Traces distribuÃ©es |

## ğŸ“ Structure du Projet

```
llm-proxy/
â”œâ”€â”€ .environments/          # Configuration Docker (postgres, redis, keycloak...)
â”œâ”€â”€ applications/
â”‚   â”œâ”€â”€ proxy/              # Application principale
â”‚   â”‚   â”œâ”€â”€ backend/        # Solution .NET 9
â”‚   â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Core/           # Domain Layer
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Application/    # Application Layer
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Infrastructure/ # Infrastructure Layer
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Presentation/   # Gateway & Admin API
â”‚   â”‚   â”‚   â””â”€â”€ tests/              # Tests unitaires
â”‚   â”‚   â””â”€â”€ frontend/       # Vue.js 3 + PrimeVue
â”‚   â””â”€â”€ authorization/      # Service d'autorisation (Azure RBAC style)
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ adr/                # Architecture Decision Records (59+ ADRs)
â”‚   â””â”€â”€ ...                 # Documentation technique
â”œâ”€â”€ k8s/                    # Manifestes Kubernetes
â””â”€â”€ scripts/                # Scripts d'automatisation
```

## ğŸ”§ Configuration

### Variables d'Environnement

CrÃ©ez un fichier `.env` Ã  la racine :

```env
# Base de donnÃ©es
POSTGRES_HOST=localhost
POSTGRES_PORT=15432
POSTGRES_USER=postgres
POSTGRES_PASSWORD=your-password
POSTGRES_DB=development

# Redis
REDIS_HOST=localhost
REDIS_PORT=6379

# Keycloak
KEYCLOAK_URL=http://localhost:8080
KEYCLOAK_REALM=development
KEYCLOAK_CLIENT_ID=llm-proxy
```

Voir [.env.example](.env.example) pour un exemple complet.

## ğŸ§ª Tests

```bash
# Backend - Tests unitaires
cd applications/proxy/backend
dotnet test

# Frontend - Tests unitaires
cd applications/proxy/frontend
npm run test:unit

# Frontend - Tests E2E
npm run test:e2e
```

## ğŸ“– Documentation

- [Architecture](docs/ARCHITECTURE.md) - Vue d'ensemble de l'architecture
- [Database](docs/DATABASE.md) - SchÃ©ma et migrations
- [ADRs](docs/adr/) - Architecture Decision Records (59+ dÃ©cisions documentÃ©es)
- [Feature Flags](docs/FEATURE_FLAGS.md) - Configuration des feature toggles

## ğŸ” SÃ©curitÃ©

- Authentification par API Key avec hachage SHA-256
- Support JWT via Keycloak
- Rate limiting configurable par utilisateur/tenant
- Audit logging complet avec JSONB metadata
- Isolation multi-tenant stricte

## ğŸ“Š ObservabilitÃ©

- **MÃ©triques** : Prometheus + Grafana dashboards
- **Traces** : OpenTelemetry â†’ Jaeger
- **Logs** : Structured logging avec Serilog

## ğŸ¤ Contribution

1. Fork le repository
2. CrÃ©er une feature branch (`git checkout -b feature/amazing-feature`)
3. Commit avec messages conventionnels (`git commit -m 'feat(scope): add amazing feature'`)
4. Push sur la branche (`git push origin feature/amazing-feature`)
5. Ouvrir une Pull Request

## ğŸ“œ Licence

Ce projet est sous licence MIT - voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.

---

*DÃ©veloppÃ© avec â¤ï¸ pour simplifier l'intÃ©gration des LLM en entreprise.*
